{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLM Project ‚Äî Email Routing (5 departments)\n",
        "\n",
        "This notebook is the **main deliverable**. It loads the dataset via `datapreparation.py`, trains a **DistilBERT** classifier, and reports **Accuracy**, **Inference Time**, and **Memory**.\n",
        "\n",
        "**Note:** Two other agents are required by the project specification (GPT-2 prompting and GPT-2 + LoRA). This notebook includes **placeholders** for them, but the implementation will be added in the `src/` step.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Environment / GPU check\n",
        "If you have an NVIDIA GPU, this project can run much faster.\n",
        "\n",
        "If training is slow:\n",
        "- verify CUDA is available (`torch.cuda.is_available()`)\n",
        "- free VRAM (close other Python notebooks / apps using the GPU)\n",
        "- reduce batch size to fit 4GB GPUs (RTX 3050 Laptop).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n",
            "CUDA available: True\n"
          ]
        }
      ],
      "source": [
        "import os, time, json, random\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import psutil\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Reproducibility\n",
        "RANDOM_SEED = 42\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "\n",
        "# Paths\n",
        "ROOT = Path('.').resolve()\n",
        "OUT_METRICS = ROOT / 'outputs' / 'metrics'\n",
        "OUT_FIG = ROOT / 'outputs' / 'figures'\n",
        "OUT_METRICS.mkdir(parents=True, exist_ok=True)\n",
        "OUT_FIG.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# =========================\n",
        "# Device selection (CPU / GPU)\n",
        "# =========================\n",
        "\n",
        "FORCE_DEVICE = None  \n",
        "FORCE_DEVICE = \"cpu\"   \n",
        "# FORCE_DEVICE = \"cuda\"  \n",
        "\n",
        "if FORCE_DEVICE is not None:\n",
        "    device = torch.device(FORCE_DEVICE)\n",
        "else:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"Device:\", device)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if device.type == \"cuda\":\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "\n",
        "def ram_mb() -> float:\n",
        "    return psutil.Process(os.getpid()).memory_info().rss / (1024**2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and prepare data (provided script)\n",
        "We **must** use `datapreparation.py` as provided in the project instructions:\n",
        "- load Hugging Face dataset `Tobi-Bueck/customer-support-tickets`\n",
        "- filter English tickets\n",
        "- keep only 5 departments\n",
        "- shuffle and split into train / val / test\n",
        "- build `label2id` and `id2label`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label distribution (train):\n",
            "Counter({'Technical Support': 6476, 'Customer Service': 3471, 'Billing and Payments': 2307, 'Sales and Pre-Sales': 655, 'General Inquiry': 340})\n",
            "Labels: ['Billing and Payments', 'Customer Service', 'General Inquiry', 'Sales and Pre-Sales', 'Technical Support']\n",
            "Train size: 13249 | Val size: 1656 | Test size: 1657\n"
          ]
        }
      ],
      "source": [
        "from datapreparation import load_and_prepare_data\n",
        "\n",
        "train_ds, val_ds, test_ds, label_list, label2id, id2label = load_and_prepare_data()\n",
        "\n",
        "print('Labels:', label_list)\n",
        "print('Train size:', len(train_ds), '| Val size:', len(val_ds), '| Test size:', len(test_ds))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quick sanity check\n",
        "We inspect a few examples and the label distribution to ensure the dataset matches the task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train label distribution:\n",
            "Counter({'Technical Support': 6476, 'Customer Service': 3471, 'Billing and Payments': 2307, 'Sales and Pre-Sales': 655, 'General Inquiry': 340})\n",
            "\n",
            "--- Example 0 ---\n",
            "queue: Customer Service\n",
            "subject: Guidance on Investment Data Analytics\n",
            "body: Is it possible to receive guidance on optimizing investments through the use of data analytics and available tools and services? I am interested in learning how to make data-driven decisions.\n",
            "\n",
            "--- Example 1 ---\n",
            "queue: Sales and Pre-Sales\n",
            "subject: \n",
            "body: Dear customer support, the data analytics tool is failing to process investment data efficiently. The problem might be due to software compatibility issues. After updating the associated software devi\n",
            "\n",
            "--- Example 2 ---\n",
            "queue: Customer Service\n",
            "subject: Concern Regarding CRM System Malfunction\n",
            "body: Dear Support Team, our marketing agency is facing issues with the Salesforce CRM system, which is disrupting our client data management process. It seems that recent software updates or integration er\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "print('Train label distribution:')\n",
        "print(Counter(train_ds['queue']))\n",
        "\n",
        "# Show a few samples\n",
        "for i in range(3):\n",
        "    ex = train_ds[i]\n",
        "    print('\\n--- Example', i, '---')\n",
        "    print('queue:', ex.get('queue'))\n",
        "    print('subject:', (ex.get('subject') or '')[:120])\n",
        "    print('body:', (ex.get('body') or '')[:200])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Text formatting + labels\n",
        "We create a single text field by concatenating subject and body, then add an integer label.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns: ['subject', 'body', 'answer', 'type', 'queue', 'priority', 'language', 'version', 'tag_1', 'tag_2', 'tag_3', 'tag_4', 'tag_5', 'tag_6', 'tag_7', 'tag_8', 'text', 'label']\n",
            "Example fields: {'queue': 'Customer Service', 'label': 1, 'text': 'Subject: Guidance on Investment Data Analytics\\nBody: Is it possible to receive guidance on optimizing investments through the use of data analytics and available tools and services? I am interested in learning how to make data-driven decisions.'}\n"
          ]
        }
      ],
      "source": [
        "def format_text(ex):\n",
        "    subject = ex.get('subject', '') or ''\n",
        "    body = ex.get('body', '') or ''\n",
        "    return f\"Subject: {subject}\\nBody: {body}\"\n",
        "\n",
        "def add_text_and_label(ds):\n",
        "    def _map(ex):\n",
        "        ex['text'] = format_text(ex)\n",
        "        ex['label'] = label2id[ex['queue']]\n",
        "        return ex\n",
        "    return ds.map(_map)\n",
        "\n",
        "train_ds2 = add_text_and_label(train_ds)\n",
        "val_ds2   = add_text_and_label(val_ds)\n",
        "test_ds2  = add_text_and_label(test_ds)\n",
        "\n",
        "print('Columns:', train_ds2.column_names)\n",
        "print('Example fields:', {k: train_ds2[0][k] for k in ['queue','label','text']})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. DistilBERT (Encoder) ‚Äî discriminative classifier\n",
        "We fine-tune `distilbert-base-uncased` for 5-way classification.\n",
        "\n",
        "### Why DistilBERT?\n",
        "- Encoder models are naturally suited for classification tasks.\n",
        "- Fine-tuning is efficient and usually yields strong accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a1fb175d88b4468952da9556fe80085",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DistilBertForSequenceClassification LOAD REPORT from: distilbert-base-uncased\n",
            "Key                     | Status     | \n",
            "------------------------+------------+-\n",
            "vocab_transform.weight  | UNEXPECTED | \n",
            "vocab_projector.bias    | UNEXPECTED | \n",
            "vocab_layer_norm.weight | UNEXPECTED | \n",
            "vocab_layer_norm.bias   | UNEXPECTED | \n",
            "vocab_transform.bias    | UNEXPECTED | \n",
            "classifier.weight       | MISSING    | \n",
            "pre_classifier.weight   | MISSING    | \n",
            "pre_classifier.bias     | MISSING    | \n",
            "classifier.bias         | MISSING    | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
            "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "MODEL_NAME = 'distilbert-base-uncased'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\",\n",
        "    num_labels=len(label_list),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenization\n",
        "Important: we set the target column name to `labels` because `Trainer` expects that name.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "81f04cc706734143b74100ddc8b8519c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1657 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_tok columns: ['input_ids', 'token_type_ids', 'attention_mask', 'labels']\n"
          ]
        }
      ],
      "source": [
        "def tokenize_batch(batch):\n",
        "    tok = tokenizer(\n",
        "        batch['text'],\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=256,\n",
        "    )\n",
        "    tok['labels'] = batch['label']\n",
        "    return tok\n",
        "\n",
        "train_tok = train_ds2.map(tokenize_batch, batched=True, remove_columns=train_ds2.column_names)\n",
        "val_tok   = val_ds2.map(tokenize_batch, batched=True, remove_columns=val_ds2.column_names)\n",
        "test_tok  = test_ds2.map(tokenize_batch, batched=True, remove_columns=test_ds2.column_names)\n",
        "\n",
        "# Torch format helps performance\n",
        "train_tok.set_format(type='torch')\n",
        "val_tok.set_format(type='torch')\n",
        "test_tok.set_format(type='torch')\n",
        "\n",
        "print('train_tok columns:', train_tok.column_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training (Transformers v5 compatible)\n",
        "Notes:\n",
        "- With small GPUs (4GB VRAM), use a smaller batch size and gradient accumulation.\n",
        "- `eval_strategy` is the Transformers v5 name (instead of `evaluation_strategy`).\n",
        "- `Trainer` in v5 no longer accepts `tokenizer=`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    return {'accuracy': accuracy_score(labels, preds)}\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=str(ROOT / 'outputs' / 'checkpoints' / 'distilbert'),\n",
        "    eval_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    logging_strategy='steps',\n",
        "    logging_steps=50,\n",
        "\n",
        "    num_train_epochs=2,\n",
        "\n",
        "    # Good defaults for RTX 3050 Laptop (4GB)\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=16,\n",
        "    gradient_accumulation_steps=2,   # effective batch ~16\n",
        "\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    dataloader_pin_memory=True,\n",
        "    dataloader_num_workers=2,\n",
        "\n",
        "    report_to='none',\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='accuracy',\n",
        "    greater_is_better=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_tok,\n",
        "    eval_dataset=val_tok,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1658' max='1658' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1658/1658 11:03, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.844028</td>\n",
              "      <td>0.879475</td>\n",
              "      <td>0.651570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.538122</td>\n",
              "      <td>0.814499</td>\n",
              "      <td>0.687198</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dac0d5e660c74afea41499b22a52bc89",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f94310a2a6c24cd684a3fa5535907f8f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "There were missing keys in the checkpoint model loaded: ['distilbert.embeddings.LayerNorm.weight', 'distilbert.embeddings.LayerNorm.bias'].\n",
            "There were unexpected keys in the checkpoint model loaded: ['distilbert.embeddings.LayerNorm.beta', 'distilbert.embeddings.LayerNorm.gamma'].\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1658, training_loss=1.7423453601335588, metrics={'train_runtime': 681.9986, 'train_samples_per_second': 38.853, 'train_steps_per_second': 2.431, 'total_flos': 1755154461834240.0, 'train_loss': 1.7423453601335588, 'epoch': 2.0})"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Evaluation on test set\n",
        "We report:\n",
        "- Accuracy\n",
        "- Inference time (total + per item)\n",
        "- RAM usage (process RSS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.687990343995172\n",
            "Inference total (s): 22.551611000002595\n",
            "RAM delta (MB): 0.23046875\n",
            "\n",
            "Classification report:\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "Billing and Payments       0.89      0.71      0.79       291\n",
            "    Customer Service       0.47      0.53      0.50       395\n",
            "     General Inquiry       0.00      0.00      0.00        29\n",
            " Sales and Pre-Sales       1.00      0.03      0.06        93\n",
            "   Technical Support       0.73      0.85      0.79       849\n",
            "\n",
            "            accuracy                           0.69      1657\n",
            "           macro avg       0.62      0.42      0.43      1657\n",
            "        weighted avg       0.70      0.69      0.67      1657\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[207  41   0   0  43]\n",
            " [ 13 209   0   0 173]\n",
            " [  1  12   0   0  16]\n",
            " [  2  60   0   3  28]\n",
            " [  9 119   0   0 721]]\n"
          ]
        }
      ],
      "source": [
        "def evaluate_on_test(trainer: Trainer, test_tok, label_list):\n",
        "    ram_before = ram_mb()\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    pred = trainer.predict(test_tok)\n",
        "    t1 = time.perf_counter()\n",
        "\n",
        "    logits = pred.predictions\n",
        "    y_true = pred.label_ids\n",
        "    y_pred = np.argmax(logits, axis=-1)\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    elapsed = t1 - t0\n",
        "\n",
        "    ram_after = ram_mb()\n",
        "\n",
        "    report = classification_report(y_true, y_pred, target_names=label_list, zero_division=0)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    result = {\n",
        "        'model': MODEL_NAME,\n",
        "        'agent': 'DistilBERT classifier',\n",
        "        'accuracy': float(acc),\n",
        "        'inference_time_sec_total': float(elapsed),\n",
        "        'inference_time_sec_per_item': float(elapsed / max(1, len(y_true))),\n",
        "        'ram_before_mb': float(ram_before),\n",
        "        'ram_after_mb': float(ram_after),\n",
        "        'ram_delta_mb': float(ram_after - ram_before),\n",
        "        'n_test': int(len(y_true)),\n",
        "    }\n",
        "    return result, report, cm\n",
        "\n",
        "# Run after training:\n",
        "result, report, cm = evaluate_on_test(trainer, test_tok, label_list)\n",
        "print('Accuracy:', result['accuracy'])\n",
        "print('Inference total (s):', result['inference_time_sec_total'])\n",
        "print('RAM delta (MB):', result['ram_delta_mb'])\n",
        "print('\\nClassification report:\\n', report)\n",
        "print('\\nConfusion matrix:\\n', cm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save results\n",
        "After evaluation, save metrics as JSON in `outputs/metrics/`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: C:\\Users\\Luc\\Documents\\ING5\\NLP\\LLM\\Project\\llm-email-router\\outputs\\metrics\\distilbert_results.json\n"
          ]
        }
      ],
      "source": [
        "# After evaluation:\n",
        "(OUT_METRICS / 'distilbert_results.json').write_text(json.dumps(result, indent=2), encoding='utf-8')\n",
        "print('Saved:', OUT_METRICS / 'distilbert_results.json')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Other required agents (to be added)\n",
        "The project requires two additional methods:\n",
        "1. **GPT-2 / DistilGPT-2 prompting** (no training)\n",
        "2. **GPT-2 + LoRA fine-tuning**\n",
        "\n",
        "We will implement them in `src/agents/` and call them from this notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe06568b",
      "metadata": {},
      "source": [
        "### Agent 1 ‚Äî Prompting (LLM zero-shot) with GPT-2 (baseline)\n",
        "\n",
        "In this section, we evaluate a *prompt-based* routing approach using a small decoder-only language model (`distilgpt2`).\n",
        "The goal is to test whether a generative LLM can infer the correct department **without any supervised training**.\n",
        "\n",
        "**Input to the agent**\n",
        "- We build a short prompt from the email `subject` and `body`.\n",
        "- The model must output one label among:\n",
        "  `Billing and Payments`, `Customer Service`, `General Inquiry`, `Sales and Pre-Sales`, `Technical Support`.\n",
        "\n",
        "**Why this baseline matters**\n",
        "- It provides a reference ‚Äúno-training‚Äù solution.\n",
        "- It is expected to be weaker than a discriminative classifier (DistilBERT fine-tuned), but it helps quantify the gap.\n",
        "\n",
        "**Compute constraints**\n",
        "- We run this agent on **CPU** to avoid saturating GPU memory (and because inference is already slow for generation).\n",
        "- We measure: accuracy, inference time, and RAM usage on a subset of 200 test samples (for faster execution)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "95e4a168",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c54f84ccb21485cb0068a42b93fa39f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/76 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPT2LMHeadModel LOAD REPORT from: distilgpt2\n",
            "Key                                        | Status     |  | \n",
            "-------------------------------------------+------------+--+-\n",
            "transformer.h.{0, 1, 2, 3, 4, 5}.attn.bias | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "({'model': 'distilgpt2',\n",
              "  'agent': 'GPT-2 prompting (CPU)',\n",
              "  'accuracy': 0.155,\n",
              "  'inference_time_sec_total': 32.21349569997983,\n",
              "  'inference_time_sec_per_item': 0.16106747849989916,\n",
              "  'ram_before_mb': 516.87109375,\n",
              "  'ram_after_mb': 2808.06640625,\n",
              "  'ram_delta_mb': 2291.1953125,\n",
              "  'n_test': 200},\n",
              " '                      precision    recall  f1-score   support\\n\\nBilling and Payments       0.15      1.00      0.27        31\\n    Customer Service       0.00      0.00      0.00        56\\n     General Inquiry       0.00      0.00      0.00         2\\n Sales and Pre-Sales       0.00      0.00      0.00        12\\n   Technical Support       0.00      0.00      0.00        99\\n\\n            accuracy                           0.15       200\\n           macro avg       0.03      0.20      0.05       200\\n        weighted avg       0.02      0.15      0.04       200\\n')"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from src.agents.gpt2_prompting import GPT2PromptingRouter, PromptingConfig\n",
        "from src.eval_utils import ram_mb, timed_predict, eval_classification\n",
        "\n",
        "# Prepare test items\n",
        "test_items = [{\"subject\": ex.get(\"subject\",\"\"), \"body\": ex.get(\"body\",\"\")} for ex in test_ds]\n",
        "y_true = [label2id[ex[\"queue\"]] for ex in test_ds]\n",
        "\n",
        "# üëá ICI : on force le device CPU (important)\n",
        "router = GPT2PromptingRouter(\n",
        "    label_list=label_list,\n",
        "    cfg=PromptingConfig(\n",
        "        model_name=\"distilgpt2\",\n",
        "        device=\"cpu\",          \n",
        "        max_new_tokens=8,\n",
        "        temperature=0.0,\n",
        "        do_sample=False,\n",
        "    )\n",
        ")\n",
        "\n",
        "ram_before = ram_mb()\n",
        "timed = timed_predict(router.predict_batch, test_items[:200])  # start with 200 for speed\n",
        "ram_after = ram_mb()\n",
        "\n",
        "y_pred = timed[\"preds\"]\n",
        "metrics = eval_classification(y_true[:200], y_pred, label_list)\n",
        "\n",
        "result_prompting = {\n",
        "    \"model\": \"distilgpt2\",\n",
        "    \"agent\": \"GPT-2 prompting (CPU)\",\n",
        "    \"accuracy\": metrics[\"accuracy\"],\n",
        "    \"inference_time_sec_total\": timed[\"total_sec\"],\n",
        "    \"inference_time_sec_per_item\": timed[\"per_item_sec\"],\n",
        "    \"ram_before_mb\": ram_before,\n",
        "    \"ram_after_mb\": ram_after,\n",
        "    \"ram_delta_mb\": ram_after - ram_before,\n",
        "    \"n_test\": timed[\"n_items\"],\n",
        "}\n",
        "\n",
        "result_prompting, metrics[\"report\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df3a307e",
      "metadata": {},
      "source": [
        "## Results ‚Äî GPT-2 Prompting baseline\n",
        "\n",
        "**Observed performance (200 test samples):**\n",
        "- Accuracy is low compared to the fine-tuned DistilBERT classifier.\n",
        "- Inference is slow because the model must *generate* text tokens for every email.\n",
        "- RAM usage is high because the GPT-2 model + tokenizer + generation cache occupy significant memory.\n",
        "\n",
        "**Interpretation**\n",
        "This confirms that a small decoder-only model (`distilgpt2`) used in a zero-shot prompting setup is not well-suited for reliable multi-class routing on this dataset.\n",
        "\n",
        "Main reasons:\n",
        "1. **No supervised learning**: the model is not trained to map emails ‚Üí queues.\n",
        "2. **Generation instability**: even with `temperature=0` and `do_sample=False`, outputs can be inconsistent.\n",
        "3. **Label format mismatch**: the model may output partial text, synonyms, or irrelevant tokens, which hurts exact label matching.\n",
        "4. **Class imbalance**: rare classes (e.g., `General Inquiry`) are especially difficult to predict without training.\n",
        "\n",
        "**Conclusion**\n",
        "Prompting with GPT-2 is a useful baseline, but it is clearly outperformed by the discriminative DistilBERT classifier fine-tuned on labeled data.\n",
        "The next step is to evaluate a *fine-tuned generative agent* (e.g., GPT-2 + LoRA) to see how much training improves routing quality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Final comparison table (to be added)\n",
        "At the end, we will build a table comparing all agents:\n",
        "- Accuracy\n",
        "- Inference time\n",
        "- Memory usage\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "build-nanogpt",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
